% Evaluation

\chapter{Evaluation} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter4} 

\lhead{Chapter 4. \emph{Evaluation}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

Game developers use personal preferences and creative programming techniques and tools to develop games with the hopes of successful market penetration. Often, in the course of development, the needs of the end user are lost. 

Evaluation can occur during various times during the design and development life cycle of a game – early, in the middle, late, and at the end. However, not all types of evaluation methods can be applied during all phases of design and development.

\section{Formative}

Formative evaluation is any evaluation that takes place before or during a project’s implementation with the aim of improving its design and performance.
It is essential for trying to understand why a program works or does not, and what other factors (internal and external) are at work during a project’s life.
Formative evaluation does require time but it significantly improves the likelihood of achieving a successful outcome through better program design

\subsection{Single-Condition Study}
Throughout the course of the design and development of the game we will be conducting studies by asking small groups of people to play our game. The main aims of this type of study is to learn about the opinion the game causes and to observe reactions of the players while they are testing it. This helps avoiding people being biased. We would like to observe the pace at which they learn the rules without being instructed in person, telling us whether the user interface is intuitive, if they find the game challenging, which would be visible in their scores and their engagement (do they try different songs over and over again or do they get bored after 10 minutes of playing?). 

\section{Summative}
Summative evaluation looks at the impact of an intervention on the target group. It is outcome-focused more than process focused. Typically, the findings are used to help decide whether a program should be adopted, continued, or modified for improvement


\subsection{Evaluation of Mood Detection system}

0.123735719652

We wanted to use neural networks to calculate valence and arousal ratings of songs using audio features we extract. 

We computed the mean error between participant ratings and network-predicted outputs across all segments of all test melodies. The network's performance total RMSE was 0.123735719652 on scale from 0 to 1. 
 predicted at an average accuracy of 54.3\% for all four segments. However, it performed better at predicting valence/arousal values for the final 30-second segment, at an average accuracy of 60\%. 

Results from the static network indicate that a network can be trained to identify statistical consistencies across audio features abstracted from music and satisfactorily predict valence/arousal values that closely match mean participant ratings.


\begin{table}
\begin{center}
\begin{tabular}{| c | c | c | c| } \hline 
 expected arousal & expected valence & predicted arousal & predicted valence \\ \hline \hline
  0.476390 0.339629
0.295385 0.241508
0.333232 0.341642
0.360882 0.434056
0.180681 0.185307
0.280778 0.284745
0.447892 0.314786
0.272754 0.216293
0.316515 0.310306
0.229543 0.224105
0.545507 0.470057
0.307956 0.331694
0.502996 0.431021
0.525432 0.501670
0.324717 0.309554
0.485072 0.465040
0.248289 0.209169
0.159978 0.274588
0.444313 0.438239
0.252907 0.217122
0.508490 0.475056
0.276301 0.279992
0.367237 0.359536
0.325705 0.265299
0.646694 0.426537
0.276581 0.332559
0.621411 0.594185
0.475480 0.407889
0.354076 0.274597
0.306607 0.288274
0.355211 0.337470
0.465739 0.443998
0.378525 0.383268
0.430519 0.435068
0.168037 0.287601
0.382486 0.404523
0.420924 0.278928
0.415644 0.400477
0.562128 0.467504
0.584892 0.474749
0.525280 0.554626
0.538577 0.483013
0.476250 0.472520
0.511305 0.528802
0.633593 0.563983
0.672227 0.693368
0.452509 0.476789
0.523316 0.488339
0.473689 0.522716
0.536469 0.505960
0.631893 0.751352

\end{tabular}
\caption{Table showing the root mean square error for training the network for given number of nodes in the hidden layer.}
\label{table:rsmetable}
\end{center}
\end{table}



0.155465908371

\subsection{Comparison to Original Songs}
In order to evaluate the quality of the gameplay generated by our program, we will test our game with songs already existing in the original Guitar Hero game and compare the output we get with its already defined levels. However, to make that possible, the music track we feed to our program must be an instrumental version of the same song as Guitar Hero's songs are mapped onto the key presses by looking at the guitar line of the song, not the main melody. We can then create statistics of correctly identified, false alarm and missed buttons.

\subsection{Melody Extraction Testing}
To evaluate our implementation of the melody extraction algorithms we can use the technique used at Music Information Evaluation eXchange, described in section 2.4.3 of the report. In particular, we can compare the performance of our implementation when tested on the samples used during MIREX to the official statistics presented in papers \cite{salamon, comparison}.

Another way of evaluating the game is creating a set of songs and generating levels for them. After that a trained Guitar Hero player can play those levels. If the buttons were consistently on time with the notes then the melody extraction and game synchronisation techniques are considered to work.

\subsection{Questionnaires}
Questionnaires are one of the most common and popular tools to gather data from a large number of people. They generally consist of a limited number of questions that ask participants to rate the effectiveness of various aspects of the activity. The questions should focus on the key points we are trying to evaluate. 

Questionnaires tend to be short in order to reduce the amount of time respondents need to complete them, and therefore increase the response rate. 

We plan on writing questions that are quantitative and generally consist of close-ended questions (tick the box, or scales), as the open ended questions tent to make data analysis and reporting more difficult.